# terrarium-webchat – making of & architecture

## Why this shape?
- Keep Terra’s host outbound-only. The worker pulls from the VPS and pushes replies; no inbound ports exposed on the LLM box.
- Reuse the terrarium-agent HTTP API and tool loop. Each harness (IRC, webchat) owns its own context and tooling, keeping the agent stateless.
- Thin VPS relay that only gates access (password + service token), fans out WebSocket events, and can be swapped or restarted without touching the LLM host.
- Frontend stays static and cache-friendly; all “intelligence” lives on the worker/agent side.

## Data flow (happy path)
```
Browser (mbabbott.com/terra)
    |  HTTPS + WS (/api/chat)
    v
nginx (VPS)
    |  proxies /terrarium/... to relay
    v
Relay (Express + ws)
    |  POST /api/chat/:id/messages (visitor)
    |  WS broadcast messages/workerState/assistantChunk
    ^  POST /api/chat/:id/agent[-chunk] (worker, service token)
    ^  GET /api/chats/open (worker poll) + /api/worker/updates WS
    |
Worker (Python, outbound-only)
    |  polls open chats, fetches history
    |  calls terrarium-agent (OpenAI API, streaming + tools)
    |  executes tools locally (site cache, GitHub cache, live fetch, websearch)
    |  posts replies/chunks + worker state back to relay
    v
terrarium-agent -> vLLM (localhost on LLM box)
```
Signals are publish-only from the LLM host toward the VPS; nothing on the VPS can initiate a connection into the LLM box.

## Tooling & content
- Cached site/GitHub content lives alongside the worker (`packages/terrarium-client/content/`), generated by `scripts/refresh_content.py`.
- Tools: site map, page fetch, site search, bio/projects, GitHub repo/readme fetch, live fetch (guarded), websearch (SearxNG).
- System prompt instructs the model to prefer cache, then GitHub, then live fetch, then web search.

## Streaming
- Agent supports SSE streaming; worker streams chunks to the relay via `/agent-chunk`.
- Relay broadcasts `assistantChunk` events over the visitor WS; frontend shows a “thinking” drawer.
- Final assistant messages are still persisted via `/agent`.

## Persistence options (chat history)
- Current: in-memory relay only. History disappears on restart.
- Lightweight persistence: add SQLite on the VPS relay (store chat/message rows, include IP/user agent). Keep in-memory for WS fan-out; write-through to disk. Optional admin-only dump endpoint for you to pull transcripts. This keeps the LLM host unchanged and preserves the publish-only pattern.
- Alternative: store transcripts on the LLM host (worker writes to local disk/db) if you’d rather keep visitor content off the VPS.

## Trust boundaries
- Service token protects worker-only routes.
- Access code gates visitor routes.
- Live fetch is allowlisted by host and size-capped; websearch hits local SearxNG.
- No inbound sockets to the LLM host; worker initiates all outbound calls.

## Ops notes
- Refresh content periodically with `scripts/refresh_content.py` (optionally `--crawl`).
- SearxNG runs via Docker Compose on the VPS; point `SEARCH_API_URL` at it.
- Relay/frontend need the streaming build (`/agent-chunk`) for live tokens.
- Worker env (`packages/terrarium-client/.env`) drives all tool endpoints and relay/agent URLs.
